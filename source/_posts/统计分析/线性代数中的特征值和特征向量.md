---
title: 线性代数中的特征值和特征向量
date: '2024-12-22 17:46:08'
updated: '2024-12-22 22:43:08'
excerpt: "对于一个方阵 A\uFEFF，如果存在一个非零向量 v\uFEFF 和一个标量 λ\uFEFF，使得 Av=λv\uFEFF，那么 λ\uFEFF 就是 A\uFEFF 的特征值（Eigenvalues），v\uFEFF 是对应的特征向量（Eigenvectors）。\n但特征值和特征向量的意义和价值不止如此。\n在几何意义上，矩阵乘以一个向量，可以看作矩阵对向量所在空间的变换（旋转、拉伸等操作）。而矩阵的特征向量和特征值则直接描述了线性变换的性质。特征向量指明了特定变换下的方向，特征值体现伸缩程度。\n特征分解（Eigendecomposition）是将矩阵表示为其特征向量和特征值的组合，即 A=PΛP−1\uFEFF，其中 P\uFEFF 是特征向量组成的矩阵，Λ\uFEFF 是特征值组成的对角矩阵。特征分解不仅能简化计算，比如在复杂运算中可降低难度，而且在主成分分析（PCA）里有着关键应用。"
categories:
  - 统计分析
permalink: /post/featured-values-and-feature-vectors-in-linear-algebra-2vkaem.html
comments: true
toc: true
---



> 对于一个方阵 $\mathbf{A}$，如果存在一个非零向量 $\vec{\mathbf{v}}$ 和一个标量 $\lambda$，使得 $\mathbf{A}\vec{\mathbf{v}}= \lambda\vec{\mathbf{v}}$，那么 $\lambda$ 就是 $\mathbf{A}$ 的特征值 **（Eigenvalues）** ，$\vec{\mathbf{v}}$ 是对应的**特征向量（Eigenvectors）** 。
>
> 但特征值和特征向量的意义和价值不止如此。
>
> 在几何意义上，矩阵乘以一个向量，可以看作矩阵对向量所在空间的变换（旋转、拉伸等操作）。而矩阵的特征向量和特征值则直接描述了线性变换的性质。特征向量指明了特定变换下的方向，特征值体现伸缩程度。
>
> 特征分解（Eigendecomposition）是将矩阵表示为其特征向量和特征值的组合，即 $\mathbf{A}= \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$，其中 $\mathbf{P}$ 是特征向量组成的矩阵，$\mathbf{Λ}$ 是特征值组成的对角矩阵。特征分解揭示了矩阵 A﻿ 的内在缩放特性。它表明矩阵 A﻿ 对向量的作用可以分解为：**先将向量转换到特征向量定义的坐标系下，特征向量就是这个新坐标系的基底，然后在各个特征向量方向上进行缩放，最后再转换回原始坐标系**。
>
> 特征分解不仅能简化计算，比如在复杂运算中可降低难度，而且在主成分分析（PCA）里有着关键应用。在PCA中，**特征值**表示了数据在对应特征向量方向上的**方差大小**。**特征向量代表了数据的主成分方向，即特征空间的基向量**。**降维过程就是**通过**选择最大特征值对应的特征向量**，将数据投影到一个较低维度的空间中。
>
> 在层次分析法中，利用完全一致的判断矩阵的特殊性质，特征向量归一化后被当做准则层的权重（各个准则的相对重要性），层次分析法算特征向量时可以使用简单的几何平均法和算术平均法算特征向量，最大特征值则用来计算判断矩阵的一致性比率。

本篇笔记会详细介绍特征值和特征向量的定义、意义、计算方法。

**笔记目录**

* 特征值和特征向量的定义
* 特征值和特征向量的意义

  * 特征向量和特征值的几何意义
  * 特征分解 (Eigendecomposition)的公式推导和理解

    * 特征分解公式推导
    * 理解特征分解公式
    * 特征分解的意义

      * 简化计算
      * 特征分解在PCA中的应用
* <span data-type="text">特征值和特征向量计算步骤：</span>
* <span data-type="text">特征值和特征向量计算的求解例子</span>

## 特征值和特征向量的定义

**定义：**

对于一个给定的 n 阶方阵 A，如果存在一个**非零**向量 $\vec{\mathbf{v}}$ 和一个标量 λ，使得以下等式成立：

$$
A\vec{\mathbf{v}}= \lambda\vec{\mathbf{v}}
$$

那么，λ 就被称为矩阵 A 的一个**特征值**，而 $\vec{\mathbf{v}}$ 就被称为矩阵 A 对应于特征值 λ 的一个**特征向量**。

## 特征值和特征向量的意义

这部分学习了

> * [【无痛线代】特征值究竟体现了矩阵的什么特征？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1TH4y1L7PV/?spm_id_from=..top_right_bar_window_history.content.click)
> * [【熟肉】线性代数的本质 - 10 - 特征向量与特征值_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ls411b7oL?spm_id_from=333.788.recommend_more_video.0&vd_source=b4a1fcb6dce305e26d8d16d9cbb71304)

### **特征向量和特征值的几何意义**

**矩阵与线性变换的关系：**

 一个 $m \times n$ 的矩阵 $\mathbf{A}$ 可以代表一个从 <u>$n$</u> 维空间到 <u>$m$</u> 维空间的线性变换。当矩阵 $\mathbf{A}$与一个向量 $\vec{\mathbf{v}}$ ($n \times 1$) 相乘时 ($\mathbf{A}\vec{\mathbf{v}}$)，就相当于对向量 $\vec{\mathbf{v}}$ 进行了线性变换，得到一个新的向量 ($m \times 1$)。线性变换可以对向量进行各种操作，包括旋转、拉伸等，这些操作都可以看作是矩阵对向量所在空间的变换。变换后的向量 $\mathbf{A}\vec{\mathbf{v}}$ 会改变方向和大小。

> 假设我们要将一个二维向量逆时针旋转45度（π/4弧度），这个线性变换可以用以下矩阵表示：
>
> $\mathbf{A} = \begin{bmatrix} \cos(45°) & -\sin(45°) \\ \sin(45°) & \cos(45°) \end{bmatrix} = \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix}$
>
> 现在，让我们对向量 $\vec{\mathbf{v}} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ 进行这个旋转变换：
>
> $\mathbf{A}\vec{\mathbf{v}} = \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} \end{bmatrix}$
>
> ​![旋转后的向量](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/%E6%97%8B%E8%BD%AC%E5%90%8E%E7%9A%84%E5%90%91%E9%87%8F-20241222204548-wr2z6l1.png)​
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # 设置中文字体
> plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
> plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号
> # 定义旋转矩阵（45度）
> theta = np.pi/4  # 45度的弧度值
> A = np.array([[np.cos(theta), -np.sin(theta)],
>               [np.sin(theta), np.cos(theta)]])
>
> # 定义原始向量
> v = np.array([[1], [0]])
>
> # 计算变换后的向量
> result = np.dot(A, v)
>
> # 可视化
> plt.figure(figsize=(12, 5))
>
> # 绘制原始向量
> plt.subplot(121)
> plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='原始向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('原始向量')
> plt.legend()
>
> # 绘制变换后的向量
> plt.subplot(122)
> plt.quiver(0, 0, result[0], result[1], angles='xy', scale_units='xy', scale=1, color='r', label='变换后的向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('旋转后的向量')
> plt.legend()
>
> plt.show()
>
> print("原始向量：")
> print(v)
> print("\n变换后的向量：")
> print(result)
>
>
> ```
>
> **例子：矩阵拉伸变换**
>
> 拉伸变换矩阵为：
>
> $\mathbf{A} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$
>
> 让我们对向量 $\vec{\mathbf{v}} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ 进行这个拉伸变换：
>
> $\mathbf{A}\vec{\mathbf{v}} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \cdot 1 + 0 \cdot 1 \\ 0 \cdot 1 + 3 \cdot 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$
>
> ​![拉伸的矩阵](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/%E6%8B%89%E4%BC%B8%E7%9A%84%E7%9F%A9%E9%98%B5-20241222204342-odkhja6.png)​
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # 设置中文字体
> plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
> plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号
> # 定义拉伸矩阵
> A = np.array([[2, 0],
>               [0, 3]])
>
> # 定义原始向量
> v = np.array([[1], [1]])
>
> # 计算变换后的向量
> result = np.dot(A, v)
>
> # 可视化
> plt.figure(figsize=(12, 5))
>
> # 绘制原始向量
> plt.subplot(121)
> plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='原始向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('原始向量')
> plt.legend()
>
> # 绘制变换后的向量
> plt.subplot(122)
> plt.quiver(0, 0, result[0], result[1], angles='xy', scale_units='xy', scale=1, color='r', label='变换后的向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('拉伸变换后的向量')
> plt.legend()
>
> plt.show()
>
> print("原始向量：")
> print(v)
> print("\n变换后的向量：")
> print(result)
>
> # 计算向量长度变化
> original_length = np.sqrt(v[0]**2 + v[1]**2)
> new_length = np.sqrt(result[0]**2 + result[1]**2)
> print(f"\n原始向量长度: {original_length[0]:.2f}")
> print(f"变换后向量长度: {new_length[0]:.2f}")
>
> ```

矩阵 $\mathbf{A}$乘以对于某些特殊的向量，它们的**方向保持不变（或反向），只是大小发生了缩放**。这些特殊的向量就叫做矩阵 $\mathbf{A}$ 的**特征向量**，而缩放的倍数就叫做**特征值**。

用数学公式表示就是：

$$
\mathbf{A}\vec{\mathbf{v}}= λ \vec{\mathbf{v}}
$$

### **特征分解 (Eigendecomposition)的公式推导和理解**

#### 特征分解公式推导

根据

$$
\mathbf{A}\vec{\mathbf{v}}= λ \vec{\mathbf{v}}
$$

如果一个 $n×n$ 的方阵 $\mathbf{A}$ 有 $n$ 个线性无关的特征向量 $\{\mathbf{v}_{1}, \mathbf{v}_{2}, ..., \mathbf{v}_{n}\}$，并且对应的特征值分别为 $\{λ_1, λ_2, ..., λ_n\}$，那么我们可以将这些特征向量按列组成一个矩阵 $\mathbf{P}$：

$$
\mathbf{P} = [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n]
$$

将这些特征值组成一个对角矩阵 $\mathbf{Λ}$：

$$
\mathbf{Λ} = \text{diag}(λ_1, λ_2, ..., λ_n) =  \begin{bmatrix}
λ_1 & 0 & ... & 0 \\
0 & λ_2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & λ_n
\end{bmatrix}
$$

根据特征向量和特征值的定义，我们有：

$$
\mathbf{A} \mathbf{v}_1 = λ_1 \mathbf{v}_1 \\
\mathbf{A} \mathbf{v}_2 = λ_2 \mathbf{v}_2 \\
... \\
\mathbf{A} \mathbf{v}_n = λ_n \mathbf{v}_n
$$

将这些等式组合起来，我们可以得到：

$$
\mathbf{A} [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n] = [λ_1\mathbf{v}_1 \ λ_2\mathbf{v}_2 \ ... \ λ_n\mathbf{v}_n]
$$

可以进一步写成：

$$
\mathbf{A} \mathbf{P} = [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n] \begin{bmatrix}
λ_1 & 0 & ... & 0 \\
0 & λ_2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & λ_n
\end{bmatrix}
$$

即：

$$
\mathbf{A} \mathbf{P} = \mathbf{P} \mathbf{Λ}
$$

由于我们假设特征向量是线性无关的，所以矩阵 $\mathbf{P}$ 是可逆的。因此，我们可以将上式两边同时右乘 $\mathbf{P}$ 的逆矩阵 $\mathbf{P}^{-1}$，得到：

$$
\mathbf{A} \mathbf{P} \mathbf{P}^{-1} = \mathbf{P} \mathbf{Λ} \mathbf{P}^{-1}
$$

最终得到特征分解的形式：

$$
\mathbf{A}= \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}
$$

> **对实对称矩阵的特征分解公式**
>
> 任意的 *N*×*N* 实对称矩阵的特征值都是实数且都有 *N* 个线性无关的特征向量。并且这些特征向量都可以正交单位化而得到一组正交且模为 1 的向量。故实对称矩阵 **A** 可被分解成
>
> $$
> \mathbf{A} =\mathbf{P} \mathbf{\Lambda } \mathbf{P} ^{-1}=\mathbf{P} \mathbf{\Lambda } \mathbf{P} ^{T}
> $$
>
> 其中 **P** 为 正交矩阵， **Λ** 为实对角矩阵。
>
>> 一个矩阵 $\mathbf{P}$ 是正交矩阵，如果它的列向量（或行向量）是正交单位向量。换句话说，矩阵 $\mathbf{P}$ 满足以下条件：
>>
>> * $\mathbf{P}^T \mathbf{P} = \mathbf{I}$，其中 $\mathbf{I}$ 是单位矩阵。
>>
>> 这意味着 $\mathbf{P}^T = \mathbf{P}^{-1}$，即 $\mathbf{P}$ 的转置等于它的逆矩阵。
>>

#### 特征分解的注意事项

* **只有方阵才能进行特征分解。**

  （A只能为n×n）

  $$
  \underbrace{\mathbf{A}}_{n \times n}\underbrace{\vec{\mathbf{v}}}_{n \times 1}=
  \lambda \underbrace{\vec{\mathbf{v}}}_{n \times 1}
  $$
* **并非所有方阵都能进行特征分解。**  只有当方阵有 $n$ 个线性无关的特征向量时，才能进行特征分解。如果一个矩阵没有足够的线性无关的特征向量，它被称为“有缺陷的” (defective)，不能进行特征分解。

#### 理解特征分解公式

公式： **$\mathbf{A} = \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$**  

* $\mathbf{P}$: 特征向量矩阵，代表一组新的基向量，这些基向量定义了一个新的坐标系。
* $\mathbf{Λ}$: 特征值对角矩阵，代表在各个特征向量方向上的缩放因子。
* $\mathbf{P}^{-1}$: $\mathbf{P}$ 的逆矩阵，将向量从原始坐标系转换到特征向量坐标系。

$\mathbf{A}\vec{\mathbf{v}}=\left(\mathbf{P\Lambda P}^{-1}\right)\vec{\mathbf{v}} = \mathbf{P}(\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}}))$  

简单理解$\mathbf{A}\vec{\mathbf{v}}$的计算结果：

​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241224114340-b21b2cx.png "简单理解：对向量v进行分解，找到特征")​

1. 将向量 $\begin{bmatrix} 2 \\ 1 \end{bmatrix}$ 表示为**特征向量的线性组合**：$1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1 \begin{bmatrix} 1 \\ 1 \end{bmatrix}$。
2. 将矩阵 A 作用于该线性组合：$A(1 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1 \begin{bmatrix} 1 \\ 1 \end{bmatrix}) = 1A\begin{bmatrix} 1 \\ 0 \end{bmatrix} + 1A\begin{bmatrix} 1 \\ 1 \end{bmatrix}$。
3. 利用**特征值和特征向量的关系**：$1(2\begin{bmatrix} 1 \\ 0 \end{bmatrix}) + 1(3\begin{bmatrix} 1 \\ 1 \end{bmatrix}) = 2 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 3 \begin{bmatrix} 1 \\ 1 \end{bmatrix}$。
4. 计算结果：$2 \begin{bmatrix} 1 \\ 0 \end{bmatrix} + 3 \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 5 \\ 3 \end{bmatrix}$。

具体变换过程：

1. $\mathbf{P}^{-1}\vec{\mathbf{v}}$: 将向量 $\vec{\mathbf{v}}$ 从原始坐标系转换到特征向量坐标系下。

    > 假设 $\vec{\mathbf{v}}$ 是原始坐标系下的一个向量。我们希望找到它在特征向量坐标系下的表示，记为 $\vec{\mathbf{x}}$。
    >
    > 根据基变换的原理，我们有：
    >
    > $\vec{\mathbf{v}} = \mathbf{P}\vec{\mathbf{x}} = \vec{\mathbf{v}}_1 x_1 + \vec{\mathbf{v}}_2 x_2 + ... + \vec{\mathbf{v}}_n x_n$
    >
    > 其中 $x_1, x_2, ..., x_n$ 是 $\vec{\mathbf{x}}$ 的坐标。
    >
    > 为了得到 $\vec{\mathbf{x}}$，我们可以在等式两边同时左乘 $\mathbf{P}^{-1}$：
    >
    > $\mathbf{P}^{-1}\vec{\mathbf{v}} = \mathbf{P}^{-1}\mathbf{P}\vec{\mathbf{x}} = \mathbf{I}\vec{\mathbf{x}} = \vec{\mathbf{x}}$
    >
    > 因此，$\mathbf{P}^{-1}\vec{\mathbf{v}}$ 的结果就是 $\vec{\mathbf{x}}$，即向量 $\vec{\mathbf{v}}$ 在特征向量坐标系下的表示。
    >

    ‍

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241224113653-9eeuzgg.png)​

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205825-l5uwkd8.png)​
2. $\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}})$: 在特征向量坐标系下，沿着各个特征向量的方向进行缩放。

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205837-wm60vgn.png)​
3. $\mathbf{P}(\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}}))$: 将缩放后的向量从特征向量坐标系转换回原始坐标系。

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205854-c45hmqa.png)​

💡**理解：**

特征分解揭示了矩阵 $\mathbf{A}$ 的内在缩放特性。它表明矩阵 $\mathbf{A}$ 对向量的作用可以分解为：先将向量转换到特征向量定义的坐标系下，特征向量就是这个新坐标系的基底，然后在各个特征向量方向上进行缩放，最后再转换回原始坐标系。**<u>特征向量</u>**​**指明了缩放的方向**，**<u>特征值</u>**​**指明了缩放的比例**。

#### 特征分解的意义

##### **简化计算**

特征分解将复杂的矩阵运算简化为对角矩阵的运算。对角矩阵的幂运算和求逆都非常简单，这在计算 $\mathbf{A}^n$ 或求 $\mathbf{A}$ 的函数（如指数、对数）时特别有用。

$$
\mathbf{A}^{n}= \mathbf{P}\mathbf{Λ}^{n}\mathbf{P}^{-1}
$$

​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205950-jfbbwzl.png)​

比如，利用特征分解计算斐波那契数列的通项公式

斐波那契数列是一个经典的数列，定义为 $F_0 = 0$, $F_1 = 1$，并且对于 $n \geq 2$，有递推关系 $F_n = F_{n-1} + F_{n-2}$。我们可以利用特征分解来推导出斐波那契数列的通项公式。

首先，我们可以将斐波那契数列的递推关系用矩阵形式表示：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} F_{n-1} \\ F_{n-2} \end{bmatrix}
$$

设矩阵 $\mathbf{A}$ 为：

$$
\mathbf{A} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}
$$

那么，我们有：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \mathbf{A}^{n-1} \begin{bmatrix} F_1 \\ F_0 \end{bmatrix}
$$

即：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \mathbf{A}^{n-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

为了找到 $\mathbf{A}^{n-1}$，我们进行特征分解。首先，计算 $\mathbf{A}$ 的特征值 $\lambda$，满足：

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

即：

$$
\det\begin{bmatrix} 1-\lambda & 1 \\ 1 & -\lambda \end{bmatrix} = (1-\lambda)(-\lambda) - 1 = \lambda^2 - \lambda - 1 = 0
$$

解这个特征方程，我们得到两个特征值：

$$
\lambda_{1}= \frac{1 + \sqrt{5}}{2}, \quad \lambda_{2}= \frac{1 - \sqrt{5}}{2}
$$

接下来，计算对应的特征向量。对于 $\lambda_1$：

$$
\begin{bmatrix} 1-\lambda_1 & 1 \\ 1 & -\lambda_1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

解这个方程组，我们得到特征向量 $\mathbf{v}_1 = \begin{bmatrix} \lambda_1 \\ 1 \end{bmatrix}$。

类似地，对于 $\lambda_2$，我们得到特征向量 $\mathbf{v}_2 = \begin{bmatrix} \lambda_2 \\ 1 \end{bmatrix}$。

构造矩阵 $\mathbf{P}$ 和对角矩阵 $\mathbf{Λ}$：

$$
\mathbf{P} = \begin{bmatrix} \lambda_1 & \lambda_2 \\ 1 & 1 \end{bmatrix}, \quad \mathbf{Λ} = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}
$$

由于 $\mathbf{A} = \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$，我们有：

$$
\mathbf{A}^{n-1} = \mathbf{P}\mathbf{Λ}^{n-1}\mathbf{P}^{-1}
$$

计算 $\mathbf{Λ}^{n-1}$：

$$
\mathbf{Λ}^{n-1} = \begin{bmatrix} \lambda_1^{n-1} & 0 \\ 0 & \lambda_2^{n-1} \end{bmatrix}
$$

然后，计算 $\mathbf{P}^{-1}$：

$$
\mathbf{P}^{-1} = \frac{1}{\lambda_1 - \lambda_2} \begin{bmatrix} 1 & -\lambda_2 \\ -1 & \lambda_1 \end{bmatrix}
$$

将这些代入 $\mathbf{A}^{n-1}$ 的表达式，并计算 $F_n$：

$$
F_n = \begin{bmatrix} 1 & 0 \end{bmatrix} \mathbf{A}^{n-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

经过计算，我们得到斐波那契数列的通项公式：

$$
F_n = \frac{1}{\sqrt{5}} \left( \left(\frac{1 + \sqrt{5}}{2}\right)^n - \left(\frac{1 - \sqrt{5}}{2}\right)^n \right)
$$

这个公式称为斐波那契数列的**Binet公式**，它提供了一种直接计算斐波那契数列中任意项的方法，而无需递归计算。这个公式也说明了为什么斐波那契数列中连续两个数的比值会随着数列的增长会越来越接近黄金分割。

##### 特征分解在PCA中的应用

PCA是一种用于降维的技术，目的在于通过找到数据中方差最大的方向来减少数据的维度，同时尽可能保留数据的主要信息。

**PCA的核心，是要寻找一组基组成的矩阵左乘原矩阵，使得**​**<u>n</u>**​**维特征映射到**​***k***​**维上**

主成分分析（PCA），可以通过<u>特征值分解</u>或<u>奇异值分解</u>来实现。PCA通过对协方差矩阵的特征分解，找到了数据的主成分方向，实现了有效的降维。特征分解是PCA实现的核心步骤。

**PCA计算步骤：**

1. **去中心化原始维度数据（只需要减均值即可，不必标准化方差）；**
2. **协方差矩阵**：

    计算数据特征的协方差矩阵 $\mathbf{C}$。这个矩阵描述了数据集中各个特征之间的线性关系。
3. **特征分解**：

    对协方差矩阵 $\mathbf{C}$ 进行特征分解，得到特征值和特征向量：

    $$
    \mathbf{C} = \mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1}
    $$

    其中，$\mathbf{P}$ 是特征向量矩阵，$\mathbf{\Lambda}$ 是对角矩阵，对角线上是特征值。

    由于协方差矩阵是对称矩阵，所以可以写成

    $$
    \mathbf{C} =\mathbf{P} \mathbf{\Lambda } \mathbf{P} ^{-1}=\mathbf{P} \mathbf{\Lambda } \mathbf{P} ^{T}
    $$
4. **选择主成分**：

    * 特征值表示了对应特征向量方向上的方差大小。选择特征值最大的几个特征向量作为主成分。
    * 这些主成分对应的数据方向保留了最多的信息。
5. **降维**：

    * 将原始数据投影到这些主成分上，得到降维后的数据。

💡**总结**

* **特征值**：在PCA中，特征值表示了数据在对应特征向量方向上的**方差大小**。
* **特征向量**：**代表了数据的主成分方向，即特征空间的基向量**。
* **降维过程**：通过**选择最大特征值对应的特征向量**，将数据投影到一个较低维度的空间中。

> 备注
>
> PCA通常使用奇异值分解（SVD）而不是直接使用特征分解。这是因为SVD在数值稳定性和计算效率上具有优势。以下是一些具体原因：
>
> SVD的优势
>
> 1. **数值稳定性**：
>
>     * SVD对所有矩阵都适用，包括非方阵，而特征分解仅适用于方阵。特征值分解只能针对方阵，如果A是一个非方阵，尤其是在PCA当中，样本的特征数量和样本数量一般都是不相同的。这时候就无法使用特征值分解的方法来计算特征向量。
>
>       * 特征值分解只能用于方阵（A只能为n×n）
>
>         $$
>         \underbrace{\mathbf{A}}_{n \times n}\underbrace{\vec{\mathbf{v}}}_{n \times 1}=
>         \lambda \underbrace{\vec{\mathbf{v}}}_{n \times 1}
>         $$
>       * SVD对所有矩阵都适用，包括非方阵
>
>         $$
>         \underbrace{\mathbf{A}}_{m×n}= \underbrace{\mathbf{U}}_{m×m}\ \underbrace{\mathbf{\Sigma}}
>         _{m×n}\ \underbrace{\mathbf{V}^T}_{n×n}
>         $$
>     * SVD在数值计算中更稳定，尤其是在处理接近奇异的矩阵时。
> 2. **计算效率**：
>
>     * 对于高维数据，直接计算协方差矩阵并进行特征分解可能会非常耗时。SVD可以直接在原始数据矩阵上操作，避免构造协方差矩阵。
>
> PCA通过SVD实现的步骤
>
> 1. **数据中心化**：
>
>     * 将数据矩阵 $\mathbf{X}$ 中的每个特征减去其均值，使数据中心化。
> 2. **SVD分解**：
>
>     * 对中心化后的数据矩阵 $\mathbf{X}$ 进行SVD：
>
>       $$
>       \mathbf{X}= \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathsf{T}}
>       $$
>
>       * $\mathbf{V}^\mathsf{T}$: 右奇异向量矩阵的转置，代表原始空间中的一组标准正交基。
>       * $\mathbf{Σ}$: 奇异值对角矩阵，代表在各个奇异值方向上的缩放因子。
>       * $\mathbf{U}$: 左奇异向量矩阵，代表目标空间中的一组标准正交基。
> 3. **选择主成分**：
>
>     * $\mathbf{\Sigma}$ 中的奇异值的平方与协方差矩阵的特征值相对应。选择最大的几个奇异值对应的列（或行）作为主成分。
> 4. **降维**：
>
>     * 使用 $\mathbf{V}$ 中的前几列（对应最大奇异值）将数据投影到低维空间。

## **特征值和特征向量计算步骤：**

1. **构造特征方程：**

    将定义式 $A\mathbf{v}= \lambda\mathbf{v}$ 移项，得到：

    $$
    A\mathbf{v} - \lambda\mathbf{v} = \mathbf{0}
    $$

    将 λ 乘以单位矩阵 I（与 A 同阶），得到：

    $$
    A\mathbf{v} - \lambda I\mathbf{v} = \mathbf{0}
    $$

    提取公因式 **v**：

    $$
    (A - \lambda I)\mathbf{v} = \mathbf{0}
    $$

    要使上式存在非零解 **v**，矩阵 $(A - \lambda I)$ 的行列式必须为零。因此，我们得到**特征方程**：

    $$
    det(A - \lambda I) = 0
    $$
2. **求解特征值：**

    计算行列式 $det(A - \lambda I)$，得到一个关于 λ 的多项式方程。解这个多项式方程，得到的根就是矩阵 A 的特征值。这个多项式称为矩阵 A 的**特征多项式**。
3. **求解特征向量：**

    对于每一个求得的特征值 λ，将其代入方程 $(A - \lambda I)\mathbf{v} = \mathbf{0}$。这是一个齐次线性方程组。解这个方程组，得到的非零解向量就是矩阵 A 对应于特征值 λ 的特征向量。

💡**总结一下：**

* **特征值 λ：**  通过解特征方程 $det(A - \lambda I) = 0$ 得到。
* **特征向量 v：**  对于每个特征值 λ，通过解齐次线性方程组 $(A - \lambda I)\mathbf{v} = \mathbf{0}$得到。

## **特征值和特征向量计算的**求解例子

现在我们用你提供的矩阵 A 来计算特征值和特征向量：

$$
A = \begin{bmatrix} 4 & 6 & 0 \\ -3 & -5 & 0 \\ -3 & -6 & 1 \end{bmatrix}
$$

### **1. 构造特征方程, 求解特征值**

$$
A - \lambda I = \begin{bmatrix} 4 & 6 & 0 \\ -3 & -5 & 0 \\ -3 & -6 & 1 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4-\lambda & 6 & 0 \\ -3 & -5-\lambda & 0 \\ -3 & -6 & 1-\lambda \end{bmatrix}
$$

计算行列式：

$$
det(A - \lambda I) =
\begin{vmatrix}
	4-\lambda & 6          & 0         \\
	-3        & -5-\lambda & 0         \\
	-3        & -6         & 1-\lambda
\end{vmatrix}
$$

由于第三列只有一个非零元素，我们可以按第三列展开：

$$
det(A - \lambda I) = (1-\lambda) \begin{vmatrix} 4-\lambda & 6 \\ -3 & -5-\lambda \end{vmatrix}
$$

> 这里利用了行列式的性质（代数余子式），具体见线性代数丨行列式

计算 2x2 行列式：

$$
\begin{vmatrix} 4-\lambda & 6 \\ -3 & -5-\lambda \end{vmatrix} = (4-\lambda)(-5-\lambda) - (6)(-3) = -20 - 4\lambda + 5\lambda + \lambda^2 + 18 = \lambda^2 + \lambda - 2
$$

所以，特征方程为：

$$
(1-\lambda)(\lambda^{2}+ \lambda - 2) = 0
$$

解特征方程：

$$
(1-\lambda)(\lambda+2)(\lambda-1) = 0
$$

得到特征值：

$$
\lambda_1 = 1, \quad \lambda_2 = -2, \quad \lambda_3 = 1
$$

> 注意，特征值可以重复。在这个例子中，λ = 1 是一个二重特征值。

### 2 **. 求解特征向量**

* **对于特征值 λ₁ = 1：**

  将 λ = 1 代入 $(A - \lambda I)\mathbf{v}= \mathbf{0}$​：

  $$
  \begin{bmatrix}
  	4-1 & 6    & 0   \\
  	-3  & -5-1 & 0   \\
  	-3  & -6   & 1-1
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	3  & 6  & 0 \\
  	-3 & -6 & 0 \\
  	-3 & -6 & 0
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	0 \\
  	0 \\
  	0
  \end{bmatrix}
  $$

  得到线性方程组：

  $$
  \begin{cases}
  	3x + 6y = 0  \\
  	-3x - 6y = 0 \\
  	-3x - 6y = 0
  \end{cases}
  $$

  这三个方程是等价的，化简得到：

  $$
  x + 2y = 0
  $$

  令 y = t，则 x = -2t。z 是自由变量，可以取任意值，令 z = s。

  所以，对应于特征值 λ₁ = 1 的特征向量为：

  $$
  \mathbf{v}_{1}=
  \begin{bmatrix}
  	-2t \\
  	t   \\
  	s
  \end{bmatrix}
  = t
  \begin{bmatrix}
  	-2 \\
  	1  \\
  	0
  \end{bmatrix}
  + s
  \begin{bmatrix}
  	0 \\
  	0 \\
  	1
  \end{bmatrix}
  $$

  由于 λ = 1 是二重特征值，我们得到了两个线性无关的特征向量。通常我们取一组基，例如当 t=1, s=0 和 t=0, s=1 时：

  $$
  \mathbf{v}_{1a}=
  \begin{bmatrix}
  	-2 \\
  	1  \\
  	0
  \end{bmatrix}, \quad \mathbf{v}_{1b}=
  \begin{bmatrix}
  	0 \\
  	0 \\
  	1
  \end{bmatrix}
  $$
* **对于特征值 λ₂ = -2：**

  将 λ = -2 代入 $$(A - \lambda I)\mathbf{v} = \mathbf{0}$$：

  $$
  \begin{bmatrix}4-(-2)&6&0 \\ -3&-5-(-2)&0 \\ -3&-6&1-(-2) \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	6  & 6  & 0 \\
  	-3 & -3 & 0 \\
  	-3 & -6 & 3
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	0 \\
  	0 \\
  	0
  \end{bmatrix}
  $$

  得到线性方程组：

  $$
  \begin{cases}6x + 6y = 0 \\ -3x - 3y = 0 \\ -3x - 6y + 3z = 0 \end{cases}
  $$

  前两个方程等价，化简得到 x + y = 0，即 y = -x。将 y = -x 代入第三个方程：

  $$
  3x - 6(-x) + 3z = 0
  $$

  $$
  3x + 6x + 3z = 0
  $$

  $$
  x + 3z = 0
  $$

  令 x = k，则 y = -k，z = -k。

  所以，对应于特征值 λ₂ = -2 的特征向量为：

  $$
  \mathbf{v}_{2} =
  \begin{bmatrix}
  	k  \\
  	-k \\
  	-k
  \end{bmatrix}
  = k
  \begin{bmatrix}
  	1  \\
  	-1 \\
  	-1
  \end{bmatrix}
  $$

  通常我们取 k = 1，得到一个特征向量：

  $$
  \mathbf{v}_{2}=
  \begin{bmatrix}
  	1  \\
  	-1 \\
  	-1
  \end{bmatrix}
  $$

### 3. 综上

* **特征值：**  $\lambda_{1}=\lambda_{2}= 1$, $\lambda_{3}= -2$
* **特征向量：**

  * 对于 $\lambda_{1}=\lambda_{2}= 1$，特征向量可以是 $\begin{bmatrix} 	-2 \\ 	1  \\ 	0 \end{bmatrix}$ 和 $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ 。
  * 对于 $\lambda_{3}= -2$，特征向量可以是 $\begin{bmatrix} 1 \\ -1 \\ -1 \end{bmatrix}$。

## 补充：AHP层次分析法与特征向量

> 对于成对比较矩阵来说，特征向量就代表着各个因素的权重
>
> 对于一致性的成对比较矩阵而言，第一列其实就是特征向量的比例
>
> 在实际应用中，由于人为设置的判断矩阵往往存在不一致性，因此需要计算特征向量来近似表示权重向量。而使用传统的特征值计算方式，计算量太大，所以层次分析法算特征向量可以使用简单的几何平均法和算术平均法算特征向量。几何平均法和算术平均法算特征向量的思想就是，用平均化来消除成对比较中的差异，算出一个平均权重。

### 为什么AHP层次分析法里用特征向量代表权重向量？

假设你正在计划一次旅行，有几个目的地可供选择。为了做出最佳决策，你需要考虑多个因素，比如**旅游成本、景点吸引力、住宿条件、餐饮质量**等。每个因素的重要性可以用权重来表示，假设这些权重分别为：$w_1, w_2, w_3, w_4$。

我们可能不知道每个因素的绝对重要性，但可以通过成对比较来了解它们之间的相对重要性。例如，如果你想比较“景点吸引力”和“旅游成本”，你可能会觉得景点吸引力比旅游成本重要5倍。我们用 $a_{21}$ 表示这个比率。

所有因素两两比较，得到一个判断矩阵：

$$
\begin{array}{c|cccc}
	             & \text{旅游成本} & \text{景点吸引力} & \text{住宿条件} & \text{餐饮质量} \\
	\hline
	\text{旅游成本}  & a_{11}      & a_{12}       & a_{13}      & a_{14}      \\
	\text{景点吸引力} & a_{21}      & a_{22}       & a_{23}      & a_{24}      \\
	\text{住宿条件}  & a_{31}      & a_{32}       & a_{33}      & a_{34}      \\
	\text{餐饮质量}  & a_{41}      & a_{42}       & a_{43}      & a_{44}      \\
\end{array}
$$

在这个背景下，我们假设 $y_2$ 代表“景点吸引力”的综合得分，可以写出如下关系式：

$$
y_2 = a_{21} * w_1 + a_{22} * w_2 + a_{23} * w_3 + a_{24} * w_4 \quad (1)
$$

这里，$a_{21}$ 是“景点吸引力”相对于“旅游成本”的重要性比率，再乘以$w_1$也就是旅游成本的权重，得到的就是“景点吸引力”的权重（当判断矩阵为一致时）。以此类推，那么可以简化为：

$$
y_{2} = 4 * w_{2} \quad (2)
$$

这表示“景点吸引力”的得分是其权重 $w_2$ 的n倍（n代表因素之和）。

为了更系统地处理所有因素，我们可以为每个因素（$y_1$ 到 $y_4$）写出类似的等式，并将它们排列成矩阵形式：

‍

$$
\begin{aligned}
	y_{1} & = a_{11}* w_{1}+ a_{12}* w_{2}+ a_{13}* w_{3}+ a_{14}* w_{4} \\
	y_{2} & = a_{21}* w_{1}+ a_{22}* w_{2}+ a_{23}* w_{3}+ a_{24}* w_{4} \\
	y_{3} & = a_{31}* w_{1}+ a_{32}* w_{2}+ a_{33}* w_{3}+ a_{34}* w_{4} \\
	y_{4} & = a_{41}* w_{1}+ a_{42}* w_{2}+ a_{43}* w_{3}+ a_{44}* w_{4} \\
\end{aligned}
$$

即

$$
Y = \begin{bmatrix}
	y_{1} \\
	y_{2} \\
	y_{3} \\
	y_{4}
\end{bmatrix}
=
\begin{bmatrix}
	a_{11} & a_{12} & a_{13} & a_{14} \\
	a_{21} & a_{22} & a_{23} & a_{24} \\
	a_{31} & a_{32} & a_{33} & a_{34} \\
	a_{41} & a_{42} & a_{43} & a_{44}
\end{bmatrix}
\begin{bmatrix}
	w_{1} \\
	w_{2} \\
	w_{3} \\
	w_{4}
\end{bmatrix} = Aw
$$

这可以表示为矩阵方程：

$$
Y = A * w
$$

其中：

* **Y** 是结果贡献的向量，代表每个因素的综合得分。
* **A** 是成对比较矩阵，描述了每个因素相对于其他因素的重要性。
* **w** 是我们想要确定的权重向量，表示每个因素的相对重要性。

又从等式 (2) 可以看出，Y 的每个元素都等于 w 的相应元素乘以一个常数。我们将此常数表示为 $λ$：

$$
Y = λ * w
$$

结合矩阵方程，我们得到：

$$
A * w = λ * w
$$

而这就是特征向量和特征值的定义！

在层次分析法（AHP）中，通过找到与成对比较矩阵的最大特征值相关联的特征向量，我们就可以获得最能代表所比较因素相对重要性的权重向量。

‍

### 为什么AHP层次分析法里可以用几何平均法和算术平均法算出特征向量

在理想情况下，如果判断矩阵具有完美的一致性，不同列与第一列只是差一个倍数，只有第一列有信息，那么矩阵的第一列归一化后实际上等于特征向量归一化的结果，且矩阵的最大特征值 $\lambda_{\text{max}}$ 就等于矩阵维度 $n$。

> 一个判断矩阵 $A = [a_{ij}]$ 是完美一致的，当且仅当满足以下条件：
>
> 对于任意三个因素 $i$, $j$ 和 $k$，如果因素 $i$ 比因素 $j$ 重要 $a_{ij}$ 倍，因素 $j$ 比因素 $k$ 重要 $a_{jk}$ 倍，那么理想情况下，因素 $i$ 应该比因素 $k$ 重要 $a_{ij} \times a_{jk}$ 倍。
>
> $$
> a_{ij} \cdot a_{jk} = a_{ik}, \quad \forall i, j, k
> $$
>
> 并且
>
> $$
> a_{ii} = 1, \quad a_{ij} = \frac{1}{a_{ji}}, \quad \forall i, j
> $$
>
> 以一个四维一致阵为例：
>
> $\begin{pmatrix}   1&a_{12}=\frac{1}{a_{21}}&a_{13}=\frac{1}{a_{31}}&a_{14}=\frac{1}{a_{41}}\\  a_{21}&1& a_{23}= a_{21}\cdot a_{13}&a_{24}= a_{21}\cdot a_{14}\\  a_{31}&a_{32}= a_{31}\cdot a_{12}&1&a_{34}= a_{31}\cdot a_{14}\\  a_{41}&a_{42}= a_{41}\cdot a_{12}&a_{43}= a_{41}\cdot a_{13}&1\\ \end{pmatrix}$
>
> python代码验证
>
> ```python
> import numpy as np
>
> # 定义矩阵元素
> a21, a31, a41 = 2, 3, 4  # 这里可以替换为实际的数值
>
> # 构建矩阵
> A = np.array([
>     [1, 1/a21, 1/a31, 1/a41],
>     [a21, 1, a21/a31, a21/a41],
>     [a31, a31/a21, 1, a31/a41],
>     [a41, a41/a21, a41/a31, 1]
> ])
>
> # 计算特征值和特征向量
> eigenvalues, eigenvectors = np.linalg.eig(A)
>
> # 找到最大特征值的索引
> max_index = np.argmax(eigenvalues)
>
> # 获取最大特征值及其对应的特征向量
> max_eigenvalue = eigenvalues[max_index]
> max_eigenvector = eigenvectors[:, max_index]
>
> # 归一化特征向量（元素和为1）
> sum_of_elements = np.sum(max_eigenvector)
> normalized_max_eigenvector = max_eigenvector / sum_of_elements
>
> print("最大特征值:", max_eigenvalue) # 4
> print("归一化的特征向量:", normalized_max_eigenvector) # [0.1 0.2 0.3 0.4]
>
> ```

例如，假设我们有一个完美一致的判断矩阵 $A$：

$$
A =
\begin{bmatrix}
	1    & 2   & 4 \\
	0.5  & 1   & 2 \\
	0.25 & 0.5 & 1
\end{bmatrix}
$$

在这个理想矩阵中，矩阵的最大特征值 $\lambda_{\text{max}}$ 就等于因素的数量 $n = 3$，第一列实际上就是特征向量的比例。每一列都有如下比例：

$$
w =
\begin{bmatrix}
	w_{1} \\
	w_{2} \\
	w_{3}
\end{bmatrix}
\propto
\begin{bmatrix}
	1    \\
	0.5  \\
	0.25
\end{bmatrix}
$$

为了得到权重向量，我们只需要对第一列进行归一化处理，即第一列每个元素除以所有元素的总和：

$$
w = \frac{1}{1 + 0.5 + 0.25}
\begin{bmatrix}
	1    \\
	0.5  \\
	0.25
\end{bmatrix}
=
\begin{bmatrix}
	0.571 \\
	0.286 \\
	0.143
\end{bmatrix}
$$

> 代码验证
>
> ```python
> import numpy as np
>
>
> def is_consistent(matrix):
>     """检查矩阵是否为完美一致的判断矩阵"""
>     n = matrix.shape[0]
>     for i in range(n):
>         for j in range(n):
>             for k in range(n):
>                 if not np.isclose(matrix[i, j] * matrix[j, k], matrix[i, k]):
>                     return False
>     return True
>
>
> def normalize_vector(vector):
>     """归一化向量，使其元素和为1"""
>     return vector / np.sum(vector)
>
>
> def main():
>     # 定义一个完美一致的判断矩阵
>     A = np.array([[1, 2, 4], [0.5, 1, 2], [0.25, 0.5, 1]])
>
>     # 检查矩阵是否为完美一致的
>     if is_consistent(A):
>         print("矩阵是完美一致的。")
>
>         # 提取第一列并归一化
>         first_column = A[:, 0]
>         normalized_first_column = normalize_vector(first_column)
>
>         # 计算特征向量
>         eigvals, eigvecs = np.linalg.eig(A)
>         max_eigval_index = np.argmax(eigvals)
>         principal_eigvec = eigvecs[:, max_eigval_index].real
>         normalized_principal_eigvec = normalize_vector(principal_eigvec)
>
>         print("归一化的第一列:", normalized_first_column)
>         print("归一化的特征向量:", normalized_principal_eigvec)
>
>         # 验证两者是否相等
>         if np.allclose(normalized_first_column, normalized_principal_eigvec):
>             print("第一列是特征向量的比例形式。")
>         else:
>             print("第一列不是特征向量的比例形式。")
>     else:
>         print("矩阵不是完美一致的。")
>
>
> if __name__ == "__main__":
>     main()
>
> ```

根本不需要去用复杂方法算特征向量。

然而，在实际应用中，由于人为设置的判断矩阵往往存在不一致性，因此需要计算特征向量来近似表示权重向量。

而计算判断矩阵 $A$ 的特征向量 $w$ 满足以下方程：

$$
A * w = \lambda * w
$$

用上面方程去算的话，又太过复杂了，需要先通过解特征方程 $det(A - \lambda I) = 0$ 得到特征值，在对于每个特征值 λ，通过解齐次线性方程组 $(A - \lambda I)\mathbf{v} = \mathbf{0}$得到特性向量

而几何平均法和算术平均法算权重向量（特征向量）的思想就是，用平均化来消除差异，算出一个平均权重，得到的平均权重就是近似的特征向量。

由于这个方法，需要矩阵具有一致性，所以需要判断矩阵的一致性比率

考虑到一致性矩阵的最大特征值 $\lambda_{\text{max}}$ 就等于矩阵的维度数 $n$

因此矩阵的一致性比率可以用如下公式定义：

$$
\text{CI}= \frac{\lambda_{\text{max}}- n}{n - 1}
$$

而最大特征根的计算公式为：

$$
λ_{max} = (1/n)∑_{i=1}^{n} (Aw)_{i}/w_{i}
$$

其实就是把计算的多个特征向量代入 $A w = \lambda * w$ 公式中，得到多个 $λ$， 最后取一个平均。
