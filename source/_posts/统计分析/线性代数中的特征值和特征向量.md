---
title: 线性代数中的特征值和特征向量
date: '2024-12-22 17:46:08'
updated: '2024-12-22 22:43:08'
excerpt: "对于一个方阵 A\uFEFF，如果存在一个非零向量 v\uFEFF 和一个标量 λ\uFEFF，使得 Av=λv\uFEFF，那么 λ\uFEFF 就是 A\uFEFF 的特征值（Eigenvalues），v\uFEFF 是对应的特征向量（Eigenvectors）。\n但特征值和特征向量的意义和价值不止如此。\n在几何意义上，矩阵乘以一个向量，可以看作矩阵对向量所在空间的变换（旋转、拉伸等操作）。而矩阵的特征向量和特征值则直接描述了线性变换的性质。特征向量指明了特定变换下的方向，特征值体现伸缩程度。\n特征分解（Eigendecomposition）是将矩阵表示为其特征向量和特征值的组合，即 A=PΛP−1\uFEFF，其中 P\uFEFF 是特征向量组成的矩阵，Λ\uFEFF 是特征值组成的对角矩阵。特征分解不仅能简化计算，比如在复杂运算中可降低难度，而且在主成分分析（PCA）里有着关键应用。"
categories:
  - 统计分析
permalink: /post/featured-values-and-feature-vectors-in-linear-algebra-2vkaem.html
comments: true
toc: true
---



> 对于一个方阵 $\mathbf{A}$，如果存在一个非零向量 $\vec{\mathbf{v}}$ 和一个标量 $\lambda$，使得 $\mathbf{A}\vec{\mathbf{v}}= \lambda\vec{\mathbf{v}}$，那么 $\lambda$ 就是 $\mathbf{A}$ 的特征值 **（Eigenvalues）** ，$\vec{\mathbf{v}}$ 是对应的**特征向量（Eigenvectors）** 。
>
> 但特征值和特征向量的意义和价值不止如此。
>
> 在几何意义上，矩阵乘以一个向量，可以看作矩阵对向量所在空间的变换（旋转、拉伸等操作）。而矩阵的特征向量和特征值则直接描述了线性变换的性质。特征向量指明了特定变换下的方向，特征值体现伸缩程度。
>
> 特征分解（Eigendecomposition）是将矩阵表示为其特征向量和特征值的组合，即 $\mathbf{A}= \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$，其中 $\mathbf{P}$ 是特征向量组成的矩阵，$\mathbf{Λ}$ 是特征值组成的对角矩阵。特征分解揭示了矩阵 A﻿ 的内在缩放特性。它表明矩阵 A﻿ 对向量的作用可以分解为：**先将向量转换到特征向量定义的坐标系下，特征向量就是这个新坐标系的基底，然后在各个特征向量方向上进行缩放，最后再转换回原始坐标系**。
>
> 特征分解不仅能简化计算，比如在复杂运算中可降低难度，而且在主成分分析（PCA）里有着关键应用。

本篇笔记会详细介绍特征值和特征向量的定义、意义、计算方法。

**笔记目录**

* 特征值和特征向量的定义
* 特征值和特征向量的意义

  * 特征向量和特征值的几何意义
  * 特征分解 (Eigendecomposition)的公式推导和理解

    * 特征分解公式推导
    * 理解特征分解公式
    * 特征分解的意义

      * 简化计算
      * 特征分解在PCA中的应用
* <span data-type="text">特征值和特征向量计算步骤：</span>
* <span data-type="text">特征值和特征向量计算的求解例子</span>

## 特征值和特征向量的定义

**定义：**

对于一个给定的 n 阶方阵 A，如果存在一个**非零**向量 $\vec{\mathbf{v}}$ 和一个标量 λ，使得以下等式成立：

$$
A\vec{\mathbf{v}}= \lambda\vec{\mathbf{v}}
$$

那么，λ 就被称为矩阵 A 的一个**特征值**，而 $\vec{\mathbf{v}}$ 就被称为矩阵 A 对应于特征值 λ 的一个**特征向量**。

## 特征值和特征向量的意义

这部分学习了

> * [【无痛线代】特征值究竟体现了矩阵的什么特征？_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1TH4y1L7PV/?spm_id_from=..top_right_bar_window_history.content.click)
> * [【熟肉】线性代数的本质 - 10 - 特征向量与特征值_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Ls411b7oL?spm_id_from=333.788.recommend_more_video.0&vd_source=b4a1fcb6dce305e26d8d16d9cbb71304)

### **特征向量和特征值的几何意义**

**矩阵与线性变换的关系：**

 一个 $m \times n$ 的矩阵 $\mathbf{A}$ 可以代表一个从 <u>$n$</u> 维空间到 <u>$m$</u> 维空间的线性变换。当矩阵 $\mathbf{A}$与一个向量 $\vec{\mathbf{v}}$ ($n \times 1$) 相乘时 ($\mathbf{A}\vec{\mathbf{v}}$)，就相当于对向量 $\vec{\mathbf{v}}$ 进行了线性变换，得到一个新的向量 ($m \times 1$)。线性变换可以对向量进行各种操作，包括旋转、缩放、剪切（错切）、投影等，这些操作都可以看作是矩阵对向量所在空间的变换。变换后的向量 $\mathbf{A}\vec{\mathbf{v}}$ 会改变方向和大小。

> 假设我们要将一个二维向量逆时针旋转45度（π/4弧度），这个线性变换可以用以下矩阵表示：
>
> $\mathbf{A} = \begin{bmatrix} \cos(45°) & -\sin(45°) \\ \sin(45°) & \cos(45°) \end{bmatrix} = \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix}$
>
> 现在，让我们对向量 $\vec{\mathbf{v}} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$ 进行这个旋转变换：
>
> $\mathbf{A}\vec{\mathbf{v}} = \begin{bmatrix} \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2} \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} \end{bmatrix}$
>
> ​![旋转后的向量](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/%E6%97%8B%E8%BD%AC%E5%90%8E%E7%9A%84%E5%90%91%E9%87%8F-20241222204548-wr2z6l1.png)​
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # 设置中文字体
> plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
> plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号
> # 定义旋转矩阵（45度）
> theta = np.pi/4  # 45度的弧度值
> A = np.array([[np.cos(theta), -np.sin(theta)],
>               [np.sin(theta), np.cos(theta)]])
>
> # 定义原始向量
> v = np.array([[1], [0]])
>
> # 计算变换后的向量
> result = np.dot(A, v)
>
> # 可视化
> plt.figure(figsize=(12, 5))
>
> # 绘制原始向量
> plt.subplot(121)
> plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='原始向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('原始向量')
> plt.legend()
>
> # 绘制变换后的向量
> plt.subplot(122)
> plt.quiver(0, 0, result[0], result[1], angles='xy', scale_units='xy', scale=1, color='r', label='变换后的向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('旋转后的向量')
> plt.legend()
>
> plt.show()
>
> print("原始向量：")
> print(v)
> print("\n变换后的向量：")
> print(result)
>
>
> ```
>
> **例子：矩阵拉伸变换**
>
> 拉伸变换矩阵为：
>
> $\mathbf{A} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$
>
> 让我们对向量 $\vec{\mathbf{v}} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ 进行这个拉伸变换：
>
> $\mathbf{A}\vec{\mathbf{v}} = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 2 \cdot 1 + 0 \cdot 1 \\ 0 \cdot 1 + 3 \cdot 1 \end{bmatrix} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}$
>
> ​![拉伸的矩阵](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/%E6%8B%89%E4%BC%B8%E7%9A%84%E7%9F%A9%E9%98%B5-20241222204342-odkhja6.png)​
>
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
>
> # 设置中文字体
> plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
> plt.rcParams['axes.unicode_minus'] = False    # 用来正常显示负号
> # 定义拉伸矩阵
> A = np.array([[2, 0],
>               [0, 3]])
>
> # 定义原始向量
> v = np.array([[1], [1]])
>
> # 计算变换后的向量
> result = np.dot(A, v)
>
> # 可视化
> plt.figure(figsize=(12, 5))
>
> # 绘制原始向量
> plt.subplot(121)
> plt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='b', label='原始向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('原始向量')
> plt.legend()
>
> # 绘制变换后的向量
> plt.subplot(122)
> plt.quiver(0, 0, result[0], result[1], angles='xy', scale_units='xy', scale=1, color='r', label='变换后的向量')
> plt.grid(True)
> plt.axis('equal')
> plt.xlim(-4, 4)
> plt.ylim(-4, 4)
> plt.title('拉伸变换后的向量')
> plt.legend()
>
> plt.show()
>
> print("原始向量：")
> print(v)
> print("\n变换后的向量：")
> print(result)
>
> # 计算向量长度变化
> original_length = np.sqrt(v[0]**2 + v[1]**2)
> new_length = np.sqrt(result[0]**2 + result[1]**2)
> print(f"\n原始向量长度: {original_length[0]:.2f}")
> print(f"变换后向量长度: {new_length[0]:.2f}")
>
> ```

矩阵 $\mathbf{A}$乘以对于某些特殊的向量，它们的**方向保持不变（或反向），只是大小发生了缩放**。这些特殊的向量就叫做矩阵 $\mathbf{A}$ 的**特征向量**，而缩放的倍数就叫做**特征值**。

用数学公式表示就是：

$$
\mathbf{A}\vec{\mathbf{v}}= λ \vec{\mathbf{v}}
$$

### **特征分解 (Eigendecomposition)的公式推导和理解**

#### 特征分解公式推导

根据

$$
\mathbf{A}\vec{\mathbf{v}}= λ \vec{\mathbf{v}}
$$

如果一个 $n×n$ 的方阵 $\mathbf{A}$ 有 $n$ 个线性无关的特征向量 $\{\mathbf{v}_{1}, \mathbf{v}_{2}, ..., \mathbf{v}_{n}\}$，并且对应的特征值分别为 $\{λ_1, λ_2, ..., λ_n\}$，那么我们可以将这些特征向量按列组成一个矩阵 $\mathbf{P}$：

$$
\mathbf{P} = [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n]
$$

将这些特征值组成一个对角矩阵 $\mathbf{Λ}$：

$$
\mathbf{Λ} = \text{diag}(λ_1, λ_2, ..., λ_n) =  \begin{bmatrix}
λ_1 & 0 & ... & 0 \\
0 & λ_2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & λ_n
\end{bmatrix}
$$

根据特征向量和特征值的定义，我们有：

$$
\mathbf{A} \mathbf{v}_1 = λ_1 \mathbf{v}_1 \\
\mathbf{A} \mathbf{v}_2 = λ_2 \mathbf{v}_2 \\
... \\
\mathbf{A} \mathbf{v}_n = λ_n \mathbf{v}_n
$$

将这些等式组合起来，我们可以得到：

$$
\mathbf{A} [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n] = [λ_1\mathbf{v}_1 \ λ_2\mathbf{v}_2 \ ... \ λ_n\mathbf{v}_n]
$$

可以进一步写成：

$$
\mathbf{A} \mathbf{P} = [\mathbf{v}_1 \ \mathbf{v}_2 \ ... \ \mathbf{v}_n] \begin{bmatrix}
λ_1 & 0 & ... & 0 \\
0 & λ_2 & ... & 0 \\
... & ... & ... & ... \\
0 & 0 & ... & λ_n
\end{bmatrix}
$$

即：

$$
\mathbf{A} \mathbf{P} = \mathbf{P} \mathbf{Λ}
$$

由于我们假设特征向量是线性无关的，所以矩阵 $\mathbf{P}$ 是可逆的。因此，我们可以将上式两边同时右乘 $\mathbf{P}$ 的逆矩阵 $\mathbf{P}^{-1}$，得到：

$$
\mathbf{A} \mathbf{P} \mathbf{P}^{-1} = \mathbf{P} \mathbf{Λ} \mathbf{P}^{-1}
$$

最终得到特征分解的形式：

$$
\mathbf{A}= \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}
$$

‍

#### 理解特征分解公式

公式： **$\mathbf{A} = \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$**  

* $\mathbf{P}$: 特征向量矩阵，代表一组新的基向量，这些基向量定义了一个新的坐标系。
* $\mathbf{Λ}$: 特征值对角矩阵，代表在各个特征向量方向上的缩放因子。
* $\mathbf{P}^{-1}$: $\mathbf{P}$ 的逆矩阵，将向量从原始坐标系转换到特征向量坐标系。

$\mathbf{A}\vec{\mathbf{v}}=\left(\mathbf{P\Lambda P}^{-1}\right)\vec{\mathbf{v}} = \mathbf{P}(\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}}))$  

变换过程：

1. $\mathbf{P}^{-1}\vec{\mathbf{v}}$: 将向量 $\vec{\mathbf{v}}$ 从原始坐标系转换到特征向量坐标系下。

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205825-l5uwkd8.png)​
2. $\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}})$: 在特征向量坐标系下，沿着各个特征向量的方向进行缩放。

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205837-wm60vgn.png)​
3. $\mathbf{P}(\mathbf{Λ}(\mathbf{P}^{-1}\vec{\mathbf{v}}))$: 将缩放后的向量从特征向量坐标系转换回原始坐标系。

    ​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205854-c45hmqa.png)​

💡**理解：**

特征分解揭示了矩阵 $\mathbf{A}$ 的内在缩放特性。它表明矩阵 $\mathbf{A}$ 对向量的作用可以分解为：先将向量转换到特征向量定义的坐标系下，特征向量就是这个新坐标系的基底，然后在各个特征向量方向上进行缩放，最后再转换回原始坐标系。**<u>特征向量</u>**​**指明了缩放的方向**，**<u>特征值</u>**​**指明了缩放的比例**。

#### 特征分解的意义

##### **简化计算**

特征分解将复杂的矩阵运算简化为对角矩阵的运算。对角矩阵的幂运算和求逆都非常简单，这在计算 $\mathbf{A}^n$ 或求 $\mathbf{A}$ 的函数（如指数、对数）时特别有用。

$$
\mathbf{A}^{n}= \mathbf{P}\mathbf{Λ}^{n}\mathbf{P}^{-1}
$$

​![image](https://fastly.jsdelivr.net/gh/Achuan-2/PicBed@pic/assets/image-20241222205950-jfbbwzl.png)​

比如，利用特征分解计算斐波那契数列的通项公式

斐波那契数列是一个经典的数列，定义为 $F_0 = 0$, $F_1 = 1$，并且对于 $n \geq 2$，有递推关系 $F_n = F_{n-1} + F_{n-2}$。我们可以利用特征分解来推导出斐波那契数列的通项公式。

首先，我们可以将斐波那契数列的递推关系用矩阵形式表示：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix} \begin{bmatrix} F_{n-1} \\ F_{n-2} \end{bmatrix}
$$

设矩阵 $\mathbf{A}$ 为：

$$
\mathbf{A} = \begin{bmatrix} 1 & 1 \\ 1 & 0 \end{bmatrix}
$$

那么，我们有：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \mathbf{A}^{n-1} \begin{bmatrix} F_1 \\ F_0 \end{bmatrix}
$$

即：

$$
\begin{bmatrix} F_n \\ F_{n-1} \end{bmatrix} = \mathbf{A}^{n-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

为了找到 $\mathbf{A}^{n-1}$，我们进行特征分解。首先，计算 $\mathbf{A}$ 的特征值 $\lambda$，满足：

$$
\det(\mathbf{A} - \lambda \mathbf{I}) = 0
$$

即：

$$
\det\begin{bmatrix} 1-\lambda & 1 \\ 1 & -\lambda \end{bmatrix} = (1-\lambda)(-\lambda) - 1 = \lambda^2 - \lambda - 1 = 0
$$

解这个特征方程，我们得到两个特征值：

$$
\lambda_{1}= \frac{1 + \sqrt{5}}{2}, \quad \lambda_{2}= \frac{1 - \sqrt{5}}{2}
$$

接下来，计算对应的特征向量。对于 $\lambda_1$：

$$
\begin{bmatrix} 1-\lambda_1 & 1 \\ 1 & -\lambda_1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}
$$

解这个方程组，我们得到特征向量 $\mathbf{v}_1 = \begin{bmatrix} \lambda_1 \\ 1 \end{bmatrix}$。

类似地，对于 $\lambda_2$，我们得到特征向量 $\mathbf{v}_2 = \begin{bmatrix} \lambda_2 \\ 1 \end{bmatrix}$。

构造矩阵 $\mathbf{P}$ 和对角矩阵 $\mathbf{Λ}$：

$$
\mathbf{P} = \begin{bmatrix} \lambda_1 & \lambda_2 \\ 1 & 1 \end{bmatrix}, \quad \mathbf{Λ} = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix}
$$

由于 $\mathbf{A} = \mathbf{P}\mathbf{Λ}\mathbf{P}^{-1}$，我们有：

$$
\mathbf{A}^{n-1} = \mathbf{P}\mathbf{Λ}^{n-1}\mathbf{P}^{-1}
$$

计算 $\mathbf{Λ}^{n-1}$：

$$
\mathbf{Λ}^{n-1} = \begin{bmatrix} \lambda_1^{n-1} & 0 \\ 0 & \lambda_2^{n-1} \end{bmatrix}
$$

然后，计算 $\mathbf{P}^{-1}$：

$$
\mathbf{P}^{-1} = \frac{1}{\lambda_1 - \lambda_2} \begin{bmatrix} 1 & -\lambda_2 \\ -1 & \lambda_1 \end{bmatrix}
$$

将这些代入 $\mathbf{A}^{n-1}$ 的表达式，并计算 $F_n$：

$$
F_n = \begin{bmatrix} 1 & 0 \end{bmatrix} \mathbf{A}^{n-1} \begin{bmatrix} 1 \\ 0 \end{bmatrix}
$$

经过计算，我们得到斐波那契数列的通项公式：

$$
F_n = \frac{1}{\sqrt{5}} \left( \left(\frac{1 + \sqrt{5}}{2}\right)^n - \left(\frac{1 - \sqrt{5}}{2}\right)^n \right)
$$

这个公式称为斐波那契数列的**Binet公式**，它提供了一种直接计算斐波那契数列中任意项的方法，而无需递归计算。

##### 特征分解在PCA中的应用

PCA是一种用于降维的技术，目的在于通过找到数据中方差最大的方向来减少数据的维度，同时尽可能保留数据的主要信息。

**PCA的核心，是要寻找一组基组成的矩阵左乘原矩阵，使得**​**<u>n</u>**​**维特征映射到**​***k***​**维上**

主成分分析（PCA），可以通过<u>特征值分解</u>或<u>奇异值分解</u>来实现。PCA通过对协方差矩阵的特征分解，找到了数据的主成分方向，实现了有效的降维。特征分解是PCA实现的核心步骤。

PCA计算步骤：

1. **标准化原始维度数据（实质只需要减均值即可，不必标准化方差）；**
2. **协方差矩阵**：

    * 计算数据特征的协方差矩阵 $\mathbf{C}$。这个矩阵描述了数据集中各个特征之间的线性关系。
3. **特征分解**：

    * 对协方差矩阵 $\mathbf{C}$ 进行特征分解，得到特征值和特征向量：

      $$
      \mathbf{C} = \mathbf{P}\mathbf{\Lambda}\mathbf{P}^{-1}
      $$
    * 其中，$\mathbf{P}$ 是特征向量矩阵，$\mathbf{\Lambda}$ 是对角矩阵，对角线上是特征值。
4. **选择主成分**：

    * 特征值表示了对应特征向量方向上的方差大小。选择特征值最大的几个特征向量作为主成分。
    * 这些主成分对应的数据方向保留了最多的信息。
5. **降维**：

    * 将原始数据投影到这些主成分上，得到降维后的数据。

**总结**

* **特征值**：在PCA中，特征值表示了数据在对应特征向量方向上的**方差大小**。
* **特征向量**：**代表了数据的主成分方向，即特征空间的基向量**。
* **降维过程**：通过**选择最大特征值对应的特征向量**，将数据投影到一个较低维度的空间中。

> 备注
>
> PCA通常使用奇异值分解（SVD）而不是直接使用特征分解。这是因为SVD在数值稳定性和计算效率上具有优势。以下是一些具体原因：
>
> SVD的优势
>
> 1. **数值稳定性**：
>
>     * 特征值分解只能针对方阵，如果A是一个非方阵，尤其是在PCA当中，样本的特征数量和样本数量一般都是不相同的。这时候就无法使用特征值分解的方法来计算特征向量。SVD对所有矩阵都适用，包括非方阵，而特征分解仅适用于方阵。
>     * SVD在数值计算中更稳定，尤其是在处理接近奇异的矩阵时。
> 2. **计算效率**：
>
>     * 对于高维数据，直接计算协方差矩阵并进行特征分解可能会非常耗时。SVD可以直接在原始数据矩阵上操作，避免构造协方差矩阵。
>
> PCA通过SVD实现的步骤
>
> 1. **数据中心化**：
>
>     * 将数据矩阵 $\mathbf{X}$ 中的每个特征减去其均值，使数据中心化。
> 2. **SVD分解**：
>
>     * 对中心化后的数据矩阵 $\mathbf{X}$ 进行SVD：
>
>       $$
>       \mathbf{X}= \mathbf{U}\mathbf{\Sigma}\mathbf{V}^{\mathsf{T}}
>       $$
>
>       * $\mathbf{V}^\mathsf{T}$: 右奇异向量矩阵的转置，代表原始空间中的一组标准正交基。
>       * $\mathbf{Σ}$: 奇异值对角矩阵，代表在各个奇异值方向上的缩放因子。
>       * $\mathbf{U}$: 左奇异向量矩阵，代表目标空间中的一组标准正交基。
> 3. **选择主成分**：
>
>     * $\mathbf{\Sigma}$ 中的奇异值的平方与协方差矩阵的特征值相对应。选择最大的几个奇异值对应的列（或行）作为主成分。
> 4. **降维**：
>
>     * 使用 $\mathbf{V}$ 中的前几列（对应最大奇异值）将数据投影到低维空间。

## **特征值和特征向量计算步骤：**

1. **构造特征方程：**

    将定义式 $A\mathbf{v}= \lambda\mathbf{v}$ 移项，得到：

    $$
    A\mathbf{v} - \lambda\mathbf{v} = \mathbf{0}
    $$

    将 λ 乘以单位矩阵 I（与 A 同阶），得到：

    $$
    A\mathbf{v} - \lambda I\mathbf{v} = \mathbf{0}
    $$

    提取公因式 **v**：

    $$
    (A - \lambda I)\mathbf{v} = \mathbf{0}
    $$

    要使上式存在非零解 **v**，矩阵 $(A - \lambda I)$ 的行列式必须为零。因此，我们得到**特征方程**：

    $$
    det(A - \lambda I) = 0
    $$
2. **求解特征值：**

    计算行列式 $det(A - \lambda I)$，得到一个关于 λ 的多项式方程。解这个多项式方程，得到的根就是矩阵 A 的特征值。这个多项式称为矩阵 A 的**特征多项式**。
3. **求解特征向量：**

    对于每一个求得的特征值 λ，将其代入方程 $(A - \lambda I)\mathbf{v} = \mathbf{0}$。这是一个齐次线性方程组。解这个方程组，得到的非零解向量就是矩阵 A 对应于特征值 λ 的特征向量。

💡**总结一下：**

* **特征值 λ：**  通过解特征方程 $det(A - \lambda I) = 0$ 得到。
* **特征向量 v：**  对于每个特征值 λ，通过解齐次线性方程组 $(A - \lambda I)\mathbf{v} = \mathbf{0}$得到。

## **特征值和特征向量计算的**求解例子

现在我们用你提供的矩阵 A 来计算特征值和特征向量：

$$
A = \begin{bmatrix} 4 & 6 & 0 \\ -3 & -5 & 0 \\ -3 & -6 & 1 \end{bmatrix}
$$

### **1. 构造特征方程, 求解特征值**

$$
A - \lambda I = \begin{bmatrix} 4 & 6 & 0 \\ -3 & -5 & 0 \\ -3 & -6 & 1 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} 4-\lambda & 6 & 0 \\ -3 & -5-\lambda & 0 \\ -3 & -6 & 1-\lambda \end{bmatrix}
$$

计算行列式：

$$
det(A - \lambda I) =
\begin{vmatrix}
	4-\lambda & 6          & 0         \\
	-3        & -5-\lambda & 0         \\
	-3        & -6         & 1-\lambda
\end{vmatrix}
$$

由于第三列只有一个非零元素，我们可以按第三列展开：

$$
det(A - \lambda I) = (1-\lambda) \begin{vmatrix} 4-\lambda & 6 \\ -3 & -5-\lambda \end{vmatrix}
$$

> 这里利用了行列式的性质（代数余子式），具体见线性代数丨行列式

计算 2x2 行列式：

$$
\begin{vmatrix} 4-\lambda & 6 \\ -3 & -5-\lambda \end{vmatrix} = (4-\lambda)(-5-\lambda) - (6)(-3) = -20 - 4\lambda + 5\lambda + \lambda^2 + 18 = \lambda^2 + \lambda - 2
$$

所以，特征方程为：

$$
(1-\lambda)(\lambda^{2}+ \lambda - 2) = 0
$$

解特征方程：

$$
(1-\lambda)(\lambda+2)(\lambda-1) = 0
$$

得到特征值：

$$
\lambda_1 = 1, \quad \lambda_2 = -2, \quad \lambda_3 = 1
$$

> 注意，特征值可以重复。在这个例子中，λ = 1 是一个二重特征值。

### 2 **. 求解特征向量**

* **对于特征值 λ₁ = 1：**

  将 λ = 1 代入 $(A - \lambda I)\mathbf{v}= \mathbf{0}$​：

  $$
  \begin{bmatrix}
  	4-1 & 6    & 0   \\
  	-3  & -5-1 & 0   \\
  	-3  & -6   & 1-1
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	3  & 6  & 0 \\
  	-3 & -6 & 0 \\
  	-3 & -6 & 0
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	0 \\
  	0 \\
  	0
  \end{bmatrix}
  $$

  得到线性方程组：

  $$
  \begin{cases}
  	3x + 6y = 0  \\
  	-3x - 6y = 0 \\
  	-3x - 6y = 0
  \end{cases}
  $$

  这三个方程是等价的，化简得到：

  $$
  x + 2y = 0
  $$

  令 y = t，则 x = -2t。z 是自由变量，可以取任意值，令 z = s。

  所以，对应于特征值 λ₁ = 1 的特征向量为：

  $$
  \mathbf{v}_{1}=
  \begin{bmatrix}
  	-2t \\
  	t   \\
  	s
  \end{bmatrix}
  = t
  \begin{bmatrix}
  	-2 \\
  	1  \\
  	0
  \end{bmatrix}
  + s
  \begin{bmatrix}
  	0 \\
  	0 \\
  	1
  \end{bmatrix}
  $$

  由于 λ = 1 是二重特征值，我们得到了两个线性无关的特征向量。通常我们取一组基，例如当 t=1, s=0 和 t=0, s=1 时：

  $$
  \mathbf{v}_{1a}=
  \begin{bmatrix}
  	-2 \\
  	1  \\
  	0
  \end{bmatrix}, \quad \mathbf{v}_{1b}=
  \begin{bmatrix}
  	0 \\
  	0 \\
  	1
  \end{bmatrix}
  $$
* **对于特征值 λ₂ = -2：**

  将 λ = -2 代入 $$(A - \lambda I)\mathbf{v} = \mathbf{0}$$：

  $$
  \begin{bmatrix}4-(-2)&6&0 \\ -3&-5-(-2)&0 \\ -3&-6&1-(-2) \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	6  & 6  & 0 \\
  	-3 & -3 & 0 \\
  	-3 & -6 & 3
  \end{bmatrix}
  \begin{bmatrix}
  	x \\
  	y \\
  	z
  \end{bmatrix}
  =
  \begin{bmatrix}
  	0 \\
  	0 \\
  	0
  \end{bmatrix}
  $$

  得到线性方程组：

  $$
  \begin{cases}6x + 6y = 0 \\ -3x - 3y = 0 \\ -3x - 6y + 3z = 0 \end{cases}
  $$

  前两个方程等价，化简得到 x + y = 0，即 y = -x。将 y = -x 代入第三个方程：

  $$
  3x - 6(-x) + 3z = 0
  $$

  $$
  3x + 6x + 3z = 0
  $$

  $$
  x + 3z = 0
  $$

  令 x = k，则 y = -k，z = -k。

  所以，对应于特征值 λ₂ = -2 的特征向量为：

  $$
  \mathbf{v}_{2} =
  \begin{bmatrix}
  	k  \\
  	-k \\
  	-k
  \end{bmatrix}
  = k
  \begin{bmatrix}
  	1  \\
  	-1 \\
  	-1
  \end{bmatrix}
  $$

  通常我们取 k = 1，得到一个特征向量：

  $$
  \mathbf{v}_{2}=
  \begin{bmatrix}
  	1  \\
  	-1 \\
  	-1
  \end{bmatrix}
  $$

### 3. 综上

* **特征值：**  $\lambda_{1}=\lambda_{2}= 1$, $\lambda_{3}= -2$
* **特征向量：**

  * 对于 $\lambda_{1}=\lambda_{2}= 1$，特征向量可以是 $\begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix}$ 和 $\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ (构成特征空间的一组基)。
  * 对于 $\lambda_{3}= -2$，特征向量可以是 $\begin{bmatrix} 1 \\ -1 \\ -1 \end{bmatrix}$。

‍
